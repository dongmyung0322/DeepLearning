{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f5e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c37e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "print(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "# 데이터를 pytorch 텐서로 변환\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc6722c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3260ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "num_classes = 3\n",
    "lr = 0.01\n",
    "num_epoches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da69485",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxRegression(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e836619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/ 100, Step: 12/12, Loss: 0.97\n",
      "Epoch: 2/ 100, Step: 12/12, Loss: 0.91\n",
      "Epoch: 3/ 100, Step: 12/12, Loss: 0.86\n",
      "Epoch: 4/ 100, Step: 12/12, Loss: 0.77\n",
      "Epoch: 5/ 100, Step: 12/12, Loss: 0.76\n",
      "Epoch: 6/ 100, Step: 12/12, Loss: 0.65\n",
      "Epoch: 7/ 100, Step: 12/12, Loss: 0.79\n",
      "Epoch: 8/ 100, Step: 12/12, Loss: 0.76\n",
      "Epoch: 9/ 100, Step: 12/12, Loss: 0.85\n",
      "Epoch: 10/ 100, Step: 12/12, Loss: 0.58\n",
      "Epoch: 11/ 100, Step: 12/12, Loss: 0.78\n",
      "Epoch: 12/ 100, Step: 12/12, Loss: 0.64\n",
      "Epoch: 13/ 100, Step: 12/12, Loss: 0.67\n",
      "Epoch: 14/ 100, Step: 12/12, Loss: 0.67\n",
      "Epoch: 15/ 100, Step: 12/12, Loss: 0.65\n",
      "Epoch: 16/ 100, Step: 12/12, Loss: 0.67\n",
      "Epoch: 17/ 100, Step: 12/12, Loss: 0.59\n",
      "Epoch: 18/ 100, Step: 12/12, Loss: 0.67\n",
      "Epoch: 19/ 100, Step: 12/12, Loss: 0.42\n",
      "Epoch: 20/ 100, Step: 12/12, Loss: 0.62\n",
      "Epoch: 21/ 100, Step: 12/12, Loss: 0.57\n",
      "Epoch: 22/ 100, Step: 12/12, Loss: 0.54\n",
      "Epoch: 23/ 100, Step: 12/12, Loss: 0.64\n",
      "Epoch: 24/ 100, Step: 12/12, Loss: 0.61\n",
      "Epoch: 25/ 100, Step: 12/12, Loss: 0.52\n",
      "Epoch: 26/ 100, Step: 12/12, Loss: 0.44\n",
      "Epoch: 27/ 100, Step: 12/12, Loss: 0.51\n",
      "Epoch: 28/ 100, Step: 12/12, Loss: 0.55\n",
      "Epoch: 29/ 100, Step: 12/12, Loss: 0.59\n",
      "Epoch: 30/ 100, Step: 12/12, Loss: 0.52\n",
      "Epoch: 31/ 100, Step: 12/12, Loss: 0.43\n",
      "Epoch: 32/ 100, Step: 12/12, Loss: 0.33\n",
      "Epoch: 33/ 100, Step: 12/12, Loss: 0.41\n",
      "Epoch: 34/ 100, Step: 12/12, Loss: 0.45\n",
      "Epoch: 35/ 100, Step: 12/12, Loss: 0.60\n",
      "Epoch: 36/ 100, Step: 12/12, Loss: 0.42\n",
      "Epoch: 37/ 100, Step: 12/12, Loss: 0.59\n",
      "Epoch: 38/ 100, Step: 12/12, Loss: 0.46\n",
      "Epoch: 39/ 100, Step: 12/12, Loss: 0.41\n",
      "Epoch: 40/ 100, Step: 12/12, Loss: 0.44\n",
      "Epoch: 41/ 100, Step: 12/12, Loss: 0.43\n",
      "Epoch: 42/ 100, Step: 12/12, Loss: 0.43\n",
      "Epoch: 43/ 100, Step: 12/12, Loss: 0.32\n",
      "Epoch: 44/ 100, Step: 12/12, Loss: 0.49\n",
      "Epoch: 45/ 100, Step: 12/12, Loss: 0.29\n",
      "Epoch: 46/ 100, Step: 12/12, Loss: 0.58\n",
      "Epoch: 47/ 100, Step: 12/12, Loss: 0.47\n",
      "Epoch: 48/ 100, Step: 12/12, Loss: 0.48\n",
      "Epoch: 49/ 100, Step: 12/12, Loss: 0.35\n",
      "Epoch: 50/ 100, Step: 12/12, Loss: 0.40\n",
      "Epoch: 51/ 100, Step: 12/12, Loss: 0.41\n",
      "Epoch: 52/ 100, Step: 12/12, Loss: 0.46\n",
      "Epoch: 53/ 100, Step: 12/12, Loss: 0.34\n",
      "Epoch: 54/ 100, Step: 12/12, Loss: 0.28\n",
      "Epoch: 55/ 100, Step: 12/12, Loss: 0.30\n",
      "Epoch: 56/ 100, Step: 12/12, Loss: 0.40\n",
      "Epoch: 57/ 100, Step: 12/12, Loss: 0.42\n",
      "Epoch: 58/ 100, Step: 12/12, Loss: 0.46\n",
      "Epoch: 59/ 100, Step: 12/12, Loss: 0.42\n",
      "Epoch: 60/ 100, Step: 12/12, Loss: 0.35\n",
      "Epoch: 61/ 100, Step: 12/12, Loss: 0.46\n",
      "Epoch: 62/ 100, Step: 12/12, Loss: 0.49\n",
      "Epoch: 63/ 100, Step: 12/12, Loss: 0.43\n",
      "Epoch: 64/ 100, Step: 12/12, Loss: 0.44\n",
      "Epoch: 65/ 100, Step: 12/12, Loss: 0.40\n",
      "Epoch: 66/ 100, Step: 12/12, Loss: 0.54\n",
      "Epoch: 67/ 100, Step: 12/12, Loss: 0.29\n",
      "Epoch: 68/ 100, Step: 12/12, Loss: 0.51\n",
      "Epoch: 69/ 100, Step: 12/12, Loss: 0.22\n",
      "Epoch: 70/ 100, Step: 12/12, Loss: 0.34\n",
      "Epoch: 71/ 100, Step: 12/12, Loss: 0.37\n",
      "Epoch: 72/ 100, Step: 12/12, Loss: 0.52\n",
      "Epoch: 73/ 100, Step: 12/12, Loss: 0.36\n",
      "Epoch: 74/ 100, Step: 12/12, Loss: 0.43\n",
      "Epoch: 75/ 100, Step: 12/12, Loss: 0.41\n",
      "Epoch: 76/ 100, Step: 12/12, Loss: 0.38\n",
      "Epoch: 77/ 100, Step: 12/12, Loss: 0.20\n",
      "Epoch: 78/ 100, Step: 12/12, Loss: 0.37\n",
      "Epoch: 79/ 100, Step: 12/12, Loss: 0.45\n",
      "Epoch: 80/ 100, Step: 12/12, Loss: 0.17\n",
      "Epoch: 81/ 100, Step: 12/12, Loss: 0.30\n",
      "Epoch: 82/ 100, Step: 12/12, Loss: 0.35\n",
      "Epoch: 83/ 100, Step: 12/12, Loss: 0.29\n",
      "Epoch: 84/ 100, Step: 12/12, Loss: 0.44\n",
      "Epoch: 85/ 100, Step: 12/12, Loss: 0.29\n",
      "Epoch: 86/ 100, Step: 12/12, Loss: 0.36\n",
      "Epoch: 87/ 100, Step: 12/12, Loss: 0.31\n",
      "Epoch: 88/ 100, Step: 12/12, Loss: 0.50\n",
      "Epoch: 89/ 100, Step: 12/12, Loss: 0.46\n",
      "Epoch: 90/ 100, Step: 12/12, Loss: 0.40\n",
      "Epoch: 91/ 100, Step: 12/12, Loss: 0.40\n",
      "Epoch: 92/ 100, Step: 12/12, Loss: 0.39\n",
      "Epoch: 93/ 100, Step: 12/12, Loss: 0.42\n",
      "Epoch: 94/ 100, Step: 12/12, Loss: 0.26\n",
      "Epoch: 95/ 100, Step: 12/12, Loss: 0.35\n",
      "Epoch: 96/ 100, Step: 12/12, Loss: 0.42\n",
      "Epoch: 97/ 100, Step: 12/12, Loss: 0.39\n",
      "Epoch: 98/ 100, Step: 12/12, Loss: 0.29\n",
      "Epoch: 99/ 100, Step: 12/12, Loss: 0.37\n",
      "Epoch: 100/ 100, Step: 12/12, Loss: 0.40\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) == 12:\n",
    "            print(f'Epoch: {epoch+1}/ {num_epoches}, Step: {i+1}/{total_step}, Loss: {loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d7d735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print('Test Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783d8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condatorch",
   "language": "python",
   "name": "condatorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
